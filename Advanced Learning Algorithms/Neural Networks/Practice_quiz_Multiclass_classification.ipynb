{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1\n",
    "\n",
    "<figure>\n",
    "<img src=\"MC1.PNG\"   style=\"width:600px;height:200px;\">\n",
    "</figure>\n",
    "\n",
    "\n",
    "For a multiclass classification task that has 4 possible outputs, the sum of all the activations adds up to 1. For a multiclass classification task that has 3 possible outputs, the sum of all the activations should add up to …\n",
    "\n",
    "\n",
    "- It will vary, depending on the input x. \n",
    "- Less than 1\n",
    "- 1\n",
    "- More than 1\n",
    "\n",
    "**<font color='magenta'>Answer: 1</font>**\n",
    "\n",
    "**<font color='magenta'>Explanation: The sum of all the softmax activations should add up to 1. One way to see this is that if $e^{z_{1}}=10$, $e^{z_{2}}=20$, $e^{z_{3}}=30$, then the sum of $a_{1} + a_{2} + a_{3}$ is equal to $\\frac{e^{z_{1}}+e^{z_{2}}+e^{z_{3}}}{e^{z_{1}}+e^{z_{2}}+e^{z_{3}}}$ which is 1.</font>**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2\n",
    "\n",
    "<figure>\n",
    "<img src=\"MC2.PNG\"   style=\"width:600px;height:200px;\">\n",
    "</figure>\n",
    "\n",
    "For multiclass classification, the cross entropy loss is used for training the model. If there are 4 possible classes for the output, and for a particular training example, the true class of the example is class 3 (y=3), then what does the cross entropy loss simplify to? [Hint: This loss should get smaller when $a_{3}$ gets larger.\n",
    "\n",
    "- $\\frac{-log(a_{1})+-log(a_{2})+-log(a_{3})+-log(a_{4})}{4}$\n",
    "- $\\frac{z_{3}}{z_{1}+z_{2}+z_{3}+z_{4}}$\n",
    "- $z_{3}$\n",
    "- $-log(a_{3})$\n",
    "\n",
    "**<font color='magenta'>Answer: $-log(a_{3})$</font>**\n",
    "\n",
    "**<font color='magenta'>Explanation: When the true label is 3, then the cross entropy loss for that training example is just the negative of the log of the activation for the third neuron of the softmax. All other terms of the cross entropy loss equation $(-log(a_{1}),-log(a_{2}), \\text{and} -log(a_{4}))$ are ignored.</font>**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 3\n",
    "\n",
    "<figure>\n",
    "<img src=\"MC3.PNG\"   style=\"width:600px;height:200px;\">\n",
    "</figure>\n",
    "\n",
    "\n",
    "For multiclass classification, the recommended way to implement softmax regression is to set from_logits=True in the loss function, and also to define the model's output layer with…\n",
    "\n",
    "\n",
    "- a 'softmax' activation\n",
    "\n",
    "- a 'linear' activation\n",
    "\n",
    "**<font color='magenta'>Answer: a 'linear' activation</font>**\n",
    "\n",
    "**<font color='magenta'>Explanation: Set the output as linear, because the loss function handles the calculation of the softmax with a more numerically stable method.</font>**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
