{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1\n",
    "\n",
    "In the training set below, what is $x_{4}^{(3)}$? Please type in the number below ((this is an integer such as $123$, no decimal points).\n",
    "\n",
    "<img align=\"left\" src=\"images/quizw2.PNG\"  style=\"width:1000px; padding: 15px; \" > \n",
    "\n",
    "**Answer: 30**\n",
    "\n",
    "**Explanation: â€‹$x_{4}^{(3)}$ is the 4th feature (4th column in the table) of the 3rd training example (3rd row in the table)**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2\n",
    "\n",
    "Which of the following are potential benefits of vectorization? Please choose the best option.\n",
    "\n",
    "\n",
    "\n",
    "- It makes your code run faster\n",
    "- It can make your code shorter \n",
    "- It allows your code to run more easily on parallel compute hardware\n",
    "- All of the above\n",
    "\n",
    "** Answer: All of the above**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 3\n",
    "\n",
    "True/False? To make gradient descent converge about twice as fast, a technique that almost always works is to double the learning rate $\\alpha$.\n",
    "\n",
    "- True\n",
    "- False\n",
    "\n",
    "**Answer: False**\n",
    "\n",
    "**Explanation: Doubling the learning rate may result in a learning rate that is too large, and cause gradient descent to fail to find the optimal values for the parameters $w$ and $b$.** "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
